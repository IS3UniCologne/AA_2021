{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining and Natural Language Processing\n",
    "\n",
    "\n",
    "This set of notes focuses on the basiscs of processing free text data.  In addition to the more structured relational data and graphs we have discussed previously, free text makes up one of the most common types of \"widely available\" data: web pages, unstructured \"comment\" fields in many relational databases, and many other easily-obtained large sources of data naturally come in free text form.  The notable difference, of course ,is that unlike the data types we have discussed before, free text lacks the \"easily extractable\" structure inherent in the previous types of data we have considered.\n",
    "\n",
    "This is not to say, of course, that free text data lacks structure.  Just the opposite: by its very definition free text usually needs to have meaning to the people who are reading that data.  But the task of actually understanding this structure is a problem that is still beyond the scope of \"generic\" data science tools, because it often involves human-level intelligence (or at least bringing to bear an enormous amount of external context) to understand the meaning behind the text.  \n",
    "This is also naturally a debatable point, and this particular perspective on free text data is one that may need to be reconsidered based upon advances in the upcoming years. \n",
    "\n",
    "\n",
    "## Free text in data science\n",
    "\n",
    "As mentioned above, the goal that we will consider here of using free text in data science is to extract some meaningful information from the text, _without_ any deep understanding of its meaning.  The reason for this is simple: extracting deep understanding from text is hard.\n",
    "\n",
    "\n",
    "### Natural language processing/understanding\n",
    "The general field of Natural Language Processing (some might differentiate between NLP and \"natural language understanding\", but for our purposes here you can think of these synonymously) looks to truly understand the structure behind free text, e.g. perform tasks like parse the sentences grammatically, describe the general entities and properties that the text is referring to, etc.  But it is easy to come up with examples of why this can be hard.  For example, consider he following example, known as a Winograd schema:\n",
    "\n",
    "> The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n",
    "\n",
    "If you read the sentence:\n",
    " \n",
    "> The city councilmen refused the demonstrators a permit because they feared violence.\n",
    "\n",
    "then it is fairly obviously that the \"they\" here refers to the city councilmen; fearing violence would not be a reason for denying someone a permit, so clearly it is the councilmen that fear violence.  On the other hand, in the sentence\n",
    "\n",
    "> The city councilmen refused the demonstrators a permit because they advocated violence.\n",
    "\n",
    "the \"they\" term clearly applies to the demonstrators (the city councilmen presumably would not advocate for violence, let along deny someone a permit because they, the councilmen, were advocating violence).  The point of all this (and this was the original point of these examples, which were originally proposed by Terry Winograd in 1972) is that when we do something \"simple\", like parse a sentence, we bring to bear an enormous amount of outside knowledge and context to this action.  Unlike say, XML documents, there is no internally-specified format that makes language unambiguous; it could be unambiguous only because of external context and knowledge, or it could even be completely ambiguous.  Or in a different vein, there can be grammatically-incorrect sentences that still have clear syntactic meaning, and there can be grammatically-correct sentences that are meaningful.\n",
    "\n",
    "\n",
    "### Free text in data science\n",
    "\n",
    "Fortunately, for many data science tasks, we can still extract considerable information from text _while only understanding it at an extremely rudimentary level_.  Consider the following two reviews for the same movie:\n",
    "\n",
    "> ... truly, a stunning exercise in large-scale filmmaking; a beautifully-assembled picture in which Abrams combines a magnificent cast with a marvelous flair for big-screen, sci-fi storytelling.\n",
    "\n",
    "and\n",
    "\n",
    "> It's loud and full of vim -- but a little hollow and heartless.\n",
    "\n",
    "Understanding that the first review is positive, while the second review is negative, doesn't take any deep understand of the language itself, it can be done by simple keyword lookup: \"stunning\" and \"marvelous\" are associated with a positive review, while \"hollow\" and \"heartless\" are associated with negative reviews.  Now, of course it's possible to use more complex language to signify a positive review while using some \"negative\" words, with statements like, \"not at all boring\".  But people don't usually write exclusively in this manner (doing so would be \"not at all clear\"), so that the general sentiment of text can still come through very easily even with a few instances where the words themselves can throw you off.\n",
    "\n",
    "\n",
    "**Terminology:** Before we begin, a quick note on terminology.  In these notes \"document\" will mean an individual group of free text data (this could be an actual document or a text field in a database).  \"Words\" or \"terms\" refer to individual tokens separated by whitespace, and additionally also refers to puncutation (so we will often separate punctuation explicitly from the surounding words.  \"Corpus\" refers to a collection of documents, and we will sometimes refer to the set of all _unique_ words/tokens in the corpus as the \"vocabulary\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods of text mining learned in the lecture are presented in this tutorial. We will deal extensively with tokenization and feature reduction methods. Then we will implement a simple text classification algorithm. We will herein examine the Bag of Word model in more detail and will implement both \"Total Count\" and \"TD-IDF\".\n",
    "For this topic we will use the Natural Language Toolkit, `nltk` library. https://www.nltk.org\n",
    "\n",
    "**Content**\n",
    "1. Tokenization\n",
    "    1. Simple Tokenizer\n",
    "    2. Sentence Tokenizer\n",
    "    3. Word Tokenizer\n",
    "2. Feature Reduction\n",
    "    1. Stop-Words\n",
    "    2. Stemming\n",
    "    3. Lemmatization\n",
    "3. Text Classification\n",
    "    1. Total Count\n",
    "    2. TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words models and TFIDF\n",
    "\n",
    "The bag of words model is by far the most common means of representing documents in data science.  Under this model, a document is described soley by the set of words (and possibly their counts) that make up the document.  All information about the actual ordering of the words is ignored.  It is essentially the so-called \"word cloud\" view of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Hello Mr. Ritter, how are you doing today? The University of Cologne is awesome. \n",
    "The exam will be quite easy. You shouldn't be worried. Are you interested in studying here?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re` is useful package in python for text processing services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr', ' Ritter, how are you doing today', ' The University of Cologne is awesome', ' \\nThe exam will be quite easy', \" You shouldn't be worried\", ' Are you interested in studying here', '']\n"
     ]
    }
   ],
   "source": [
    "# Trivial approach to tokenize text with regular expressions\n",
    "import re \n",
    "tokenized_text=re.split(\"\\.|\\?\", text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Ritter, how are you doing today?', 'The University of Cologne is awesome.', 'The exam will be quite easy.', \"You shouldn't be worried.\", 'Are you interested in studying here?']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization \n",
    "\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenized_text = sent_tokenize(text)\n",
    "\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Ritter', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'University', 'of', 'Cologne', 'is', 'awesome', '.', 'The', 'exam', 'will', 'be', 'quite', 'easy', '.', 'You', 'should', \"n't\", 'be', 'worried', '.', 'Are', 'you', 'interested', 'in', 'studying', 'here', '?']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 31 samples and 37 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Analyze frequency distribution of words\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "In the lecture we learned that we use a dedicated feature dimension (i.e., column) for each word. However, most languages consist of several hundred thousand up to millions of words. Accordingly, the feature space becomes very large and we quickly reach our limits (e.g., Curse of Dimensionality). Text Mining offers us methods to reduce the number of dimensions. In the following we will look at \"Stop-Word\", \"Stemming\" and \"Lemmatization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'more', 'of', 'on', 'are', 'no', 'some', \"hadn't\", 'at', 'again', 'haven', 'my', 'to', 'ain', 'this', 'whom', \"needn't\", 'yourself', 'yourselves', 'who', 'them', 'his', \"doesn't\", 'into', 't', 'ourselves', \"mightn't\", 'shan', 'above', \"that'll\", 'very', 'with', 'doesn', \"you'd\", 'she', 'but', 'aren', 'll', 'a', 'how', 'not', 'off', 'our', 'themselves', 'all', 're', 'through', 'been', 'will', 'most', 'we', 'here', 'i', 'in', 'what', 'further', 'now', 'yours', 'd', \"didn't\", 'then', 'is', 'couldn', \"it's\", \"haven't\", 'hers', 'itself', 'doing', 'too', 'from', 'an', \"weren't\", 'these', 'while', 'where', 'me', \"you've\", 'her', 'weren', 'wasn', 'once', 'for', \"couldn't\", 'each', \"won't\", 'wouldn', 'mustn', 'did', 'having', 'after', 'being', 'such', 'any', 'why', 'didn', \"you're\", \"mustn't\", 'had', 'between', 'if', 'nor', 'which', 'hasn', 'be', 'ma', 'or', 'mightn', 'below', 'up', 'out', 'because', 'during', 'won', \"wasn't\", 'those', 'can', \"aren't\", 'shouldn', 'until', 'both', 'myself', 'just', 'against', 'down', 'under', 'same', \"hasn't\", 'by', \"shouldn't\", 'own', 'has', 'y', \"don't\", \"shan't\", 'so', 'does', 'o', 'before', 'about', 'theirs', 's', 'when', 'the', 'hadn', 'have', 'needn', 'over', 'was', 'don', 'than', 'isn', 'am', 'as', 'were', \"she's\", 'himself', 'there', 'its', \"you'll\", \"should've\", 'and', 'it', 'your', 'that', 'he', 'm', \"isn't\", 'you', 've', 'their', \"wouldn't\", 'they', 'only', 'herself', 'other', 'few', 'do', 'should', 'him'}\n"
     ]
    }
   ],
   "source": [
    "# Load Stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence:\n",
      "  ['Hello', 'Mr.', 'Ritter', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'University', 'of', 'Cologne', 'is', 'awesome', '.', 'The', 'exam', 'will', 'be', 'quite', 'easy', '.', 'You', 'should', \"n't\", 'be', 'worried', '.', 'Are', 'you', 'interested', 'in', 'studying', 'here', '?']\n",
      "\n",
      " Filterd Sentence:\n",
      "  ['Hello', 'Mr.', 'Ritter', ',', 'today', '?', 'The', 'University', 'Cologne', 'awesome', '.', 'The', 'exam', 'quite', 'easy', '.', 'You', \"n't\", 'worried', '.', 'Are', 'interested', 'studying', '?']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stop words\n",
    "filtered_sent=[]\n",
    "for w in tokenized_word: # tokenized words from my input text\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w) # use this word in machine learning tasks\n",
    "print(\"Tokenized Sentence:\\n \",tokenized_word)\n",
    "print(\"\\n Filterd Sentence:\\n \",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Ritter', ',', 'today', '?', 'The', 'University', 'Cologne', 'awesome', '.', 'The', 'exam', 'quite', 'easy', '.', 'You', \"n't\", 'worried', '.', 'Are', 'interested', 'studying', '?']\n",
      "Stemmed Sentence: ['hello', 'mr.', 'ritter', ',', 'today', '?', 'the', 'univers', 'cologn', 'awesom', '.', 'the', 'exam', 'quit', 'easi', '.', 'you', \"n't\", 'worri', '.', 'are', 'interest', 'studi', '?']\n"
     ]
    }
   ],
   "source": [
    "# Stemming - Reduce a word into its word root, that meaning, cut off the suffixes. \n",
    "# This could lead to stems which are not grammatically correct.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer() # https://de.wikipedia.org/wiki/Porter-Stemmer-Algorithmus\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Ritter', ',', 'today', '?', 'The', 'University', 'Cologne', 'awesome', '.', 'The', 'exam', 'quite', 'easy', '.', 'You', \"n't\", 'worried', '.', 'Are', 'interested', 'studying', '?']\n",
      "Lemmatized Sentence: ['Hello', 'Mr.', 'Ritter', ',', 'today', '?', 'The', 'University', 'Cologne', 'awesome', '.', 'The', 'exam', 'quite', 'easy', '.', 'You', \"n't\", 'worry', '.', 'Are', 'interest', 'study', '?']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization - Reduces a word to its base word. \n",
    "# This method is more sophisticated than stemming since it leverage contextual information and a dictionary.\n",
    "# However, result may depend on the chosen dictionary.\n",
    "\n",
    "#nltk.download('wordnet') # WordNet is just another NLTK corpus reader, and can be imported like this:\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words=[]\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w,\"v\"))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Lemmatized Sentence:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the obvious information we are throwing away with this representation, it send to work surpringingly well in practice, for the precise reason we mentioned above, that the general \"gist\" of many documents can be obtained with only looking at the presence/absence of words in the text.\n",
    "\n",
    "In these notes, we'll cover a simple example of creating so-called TFIDF vectors, which represent the documents via a (weighted) bag of words model. We will then using these to compute the similarity between documents.  This technique is a common approach for applications like document retrieval and or search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency\n",
    "\n",
    "In our example, let's begin with our corpus that contains the following three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"the goal of this lecture is to explain the basics of free text processing\",\n",
    "             \"the bag of words model is one such approach\",\n",
    "             \"text processing via bag of words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting, we can represent the documents using a _term frequency_ matrix, and $m \\times n$ matrix where $m$ denotes the number of documents, and $n$ denotes the vocabulary size (i.e., the number of unique words across all documents).  To see (the naive way of) how to construct this list, let's first consider a simple way to get a list of all unique words across all documents.  In general there is no need to actually sort the list of words, but we will do so for simplicity here.  It's a good idea to also generate a dictionary that maps words to their index in this list, as we'll frequently want to look up the index corresponding to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['approach', 'bag', 'basics', 'explain', 'free', 'goal', 'is', 'lecture', 'model', 'of', 'one', 'processing', 'such', 'text', 'the', 'this', 'to', 'via', 'words'] \n",
      "\n",
      "{'approach': 0, 'bag': 1, 'basics': 2, 'explain': 3, 'free': 4, 'goal': 5, 'is': 6, 'lecture': 7, 'model': 8, 'of': 9, 'one': 10, 'processing': 11, 'such': 12, 'text': 13, 'the': 14, 'this': 15, 'to': 16, 'via': 17, 'words': 18} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_words = [doc.split() for doc in documents]\n",
    "vocab = sorted(set(sum(document_words, [])))\n",
    "vocab_dict = {k:i for i,k in enumerate(vocab)}\n",
    "print(vocab, \"\\n\")\n",
    "print(vocab_dict, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct a matrix that contains word counts (term frequencies) for all the documents.  We'll also refer to the term frequency of the $j$th word in the $i$th library as $\\mathrm{tf}_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 1 1 0 2 0 1 0 1 2 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_tf = np.zeros((len(documents), len(vocab)), dtype=int)\n",
    "for i,doc in enumerate(document_words):\n",
    "    for word in doc:\n",
    "        X_tf[i, vocab_dict[word]] += 1\n",
    "print(X_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, each of the _rows_ in this matrix correponds to one of the three documents above, and each column correponds to one of the 19 words.  Importantly, note that in practice (for instance on the homework), you will want to create term frequency matrices directly in sparse format, because the term frequency matrix is itself typically sparse (when there are a large number of documents, many words will only be contained in a small number of the document).\n",
    "\n",
    "In this case, we had the entry in our term frequency matrix just correspond to the number of occurences of that term.  But there are other possibilities as well:\n",
    "\n",
    "* The entry can be binary: 1 if the term occurs (any number of times), and 0 otherwise.  This somewhat mitigates the significance of common words that may occur very frequently.\n",
    "* A nonlinear scaling, e.g. $\\log (1+\\mathrm{tf}_{i,j})$, which lies somewhere between the binary case and the raw counts.\n",
    "* A scaled version of term frequencies, e.g., scaling by the maximum term frequency in the document, $\\mathrm{tf}_{i,j} / \\max_k \\mathrm{tf}_{i,k}$.  (Note that this won't affect the actual similarity scores we'll discuss next, because we will ultimately scale each document but it affects the term frequency matrix itself, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse document frequency\n",
    "\n",
    "An obvious issue with using normal term frequency counts to represent a document is that the document's vector (and the resulting similarities we will consider) will often be \"dominated\" by very common words, for example: \"of\", \"the\", \"is\", in the preceeding example documents.  This issue can be mitigated to some extent by excluding so-called \"stop words\" (common English words like \"the\", \"a\", \"of\" that aren't considered relevant to the particular documents) from the term frequency matrix.  But this still ignores the case where a word that may not be a generic stop word still happens to occur in a very large number of documents.  Intuitively, we expect that the most \"important\" words in a document are precisely those that only occur in some relatively small number of documents, so that we want to discount the weight of very frequently-occurring terms.\n",
    "\n",
    "This can be accomplished via the inverse document frequency weight for words.  Just as with term frequencies, there are some different weightings of this term, but the most common formulation is\n",
    "\\begin{equation}\n",
    "\\mathrm{idf}_j = \\log\\left(\\frac{\\mbox{# documents}}{\\mbox{# documents with word $j$}}\\right).\n",
    "\\end{equation}\n",
    "As an example, if the word is contained in every document, then the inverse document frequency weight will be zero (log of one).  In contrast, if a word occurs in only one document, its inverse document frequency will be $\\log (\\mbox{# documents})$.\n",
    "\n",
    "Note that inverse document frequency is a _per word_ term, as opposed to term frequency, which is _per word and document_.  We can compute inverse document frequency for our data set as follows, which mainly just requires counting how many documents contain each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.09861229 0.40546511 1.09861229 1.09861229 1.09861229 1.09861229\n",
      " 0.40546511 1.09861229 1.09861229 0.         1.09861229 0.40546511\n",
      " 1.09861229 0.40546511 0.40546511 1.09861229 1.09861229 1.09861229\n",
      " 0.40546511]\n"
     ]
    }
   ],
   "source": [
    "idf = np.log(X_tf.shape[0]/X_tf.astype(bool).sum(axis=0))\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "The term frequency inverse document frequency (TFIDF) combination simply scales the columns of the term frequency matrix by their inverse document frequency.  In doing so, we still have an effective bag of words representation of each document, but we do so with the weighting implied by the inverse document frequency: discouting words that occur very frequently, and increasing the weight of less frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  0.40546511 1.09861229 0.         0.         0.         0.40546511\n",
      "  0.         0.40546511 0.81093022 1.09861229 1.09861229 0.\n",
      "  0.        ]\n",
      " [1.09861229 0.40546511 0.         0.         0.         0.\n",
      "  0.40546511 0.         1.09861229 0.         1.09861229 0.\n",
      "  1.09861229 0.         0.40546511 0.         0.         0.\n",
      "  0.40546511]\n",
      " [0.         0.40546511 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40546511\n",
      "  0.         0.40546511 0.         0.         0.         1.09861229\n",
      "  0.40546511]]\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = X_tf * idf\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Given a TFIDF (or just term frequency) matrix, one of the more common questions to address is to compute similarity between multiple documents in the corpus.  The common metric for doing so is to compute the cosine similarity between two different documents.  This is simply a normalize inner product between the vectors describing each documents.  Specifically,\n",
    "\\begin{equation}\n",
    "\\mbox{CosineSimilarity}(x,y) = \\frac{x^T y}{\\|x\\|_2 \\cdot \\|y\\|_2}.\n",
    "\\end{equation}\n",
    "The cosine similarity is a number between zero (meaning the two documents share no terms in common) and one (meaning the two documents have the exact same term frequency or TFIDF representation).  In fact, the cosine similarity is exactly the converse of the squared Eucliean distance between the normalized document vectors; formally, for $\\tilde{x} = x / \\|x\\|_2$ and $\\tilde{y} = y / \\|y\\|_2$, \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{1}{2}\\|\\tilde{x} - \\tilde{y}\\|_2^2 & = \\frac{1}{2}(\\tilde{x} - \\tilde{y})^T (\\tilde{x} - \\tilde{y}) \\\\\n",
    "& = \\frac{1}{2} (\\tilde{x}^T \\tilde{x} - 2 \\tilde{x}^T \\tilde{y} + \\tilde{y}^T \\tilde{y}) \\\\\n",
    "& = \\frac{1}{2} (1 - 2 \\tilde{x}^T \\tilde{y} + 1) \\\\\n",
    "& = 1 - \\mbox{CosineSimilarity}(x,y).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We can compute cosine similarity between the TFIDF vectors in our corpus as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.06796739 0.07771876]\n",
      " [0.06796739 1.         0.10281225]\n",
      " [0.07771876 0.10281225 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_tfidf_norm = X_tfidf / np.linalg.norm(X_tfidf, axis=1)[:,None]\n",
    "M = X_tfidf_norm @ X_tfidf_norm.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at the cosine similarity with the ordinary term frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.3928371  0.38490018]\n",
      " [0.3928371  1.         0.40824829]\n",
      " [0.38490018 0.40824829 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_tf_norm = X_tf / np.linalg.norm(X_tf, axis=1)[:,None]\n",
    "M = X_tf_norm @ X_tf_norm.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, using the term frequency matrix results in substantially higher scores: the inclusion of words that occur across many of the documents add positive terms to the inner product between the document vectors, resulting in higher similarity scores.  Note, however, that the distances are typically _all_ scaled up, meaning that the relative distances are not necssarily any more informative than the TFIDF similarities (indeed, it is typically less informative, since the scores are inflated by the \"random\" occurance of multiple high-frequency words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Text Classification for Movie Data\n",
    "\n",
    "**Objective**\n",
    "Compute the sentiment (i.e., if text has a postive or negative connotation) of text documents. \n",
    "\n",
    "Example: News Article about politicians or companies. Might be used to identify \"shitstorms\" or predict stock prices (www.stockpulse.com).\n",
    "\n",
    "**Steps**\n",
    "- Tokenization\n",
    "- Preprocessing (Remove Stop Words, Stemming and Lemmatization)\n",
    "- Bag of Words Model\n",
    "- TF-IDF\n",
    "- Model Building\n",
    "- Model Evaluation\n",
    "\n",
    "*Data Source*\n",
    "Rotten Tomato Movie Reviews\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "data=pd.read_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>what</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0          1           1  A series of escapades demonstrating the adage ...   \n",
       "1          2           1  A series of escapades demonstrating the adage ...   \n",
       "2          3           1                                           A series   \n",
       "3          4           1                                                  A   \n",
       "4          5           1                                             series   \n",
       "5          6           1  of escapades demonstrating the adage that what...   \n",
       "6          7           1                                                 of   \n",
       "7          8           1  escapades demonstrating the adage that what is...   \n",
       "8          9           1                                          escapades   \n",
       "9         10           1  demonstrating the adage that what is good for ...   \n",
       "10        11           1                            demonstrating the adage   \n",
       "11        12           1                                      demonstrating   \n",
       "12        13           1                                          the adage   \n",
       "13        14           1                                                the   \n",
       "14        15           1                                              adage   \n",
       "15        16           1                    that what is good for the goose   \n",
       "16        17           1                                               that   \n",
       "17        18           1                         what is good for the goose   \n",
       "18        19           1                                               what   \n",
       "19        20           1                              is good for the goose   \n",
       "\n",
       "    Sentiment  \n",
       "0           1  \n",
       "1           2  \n",
       "2           2  \n",
       "3           2  \n",
       "4           2  \n",
       "5           2  \n",
       "6           2  \n",
       "7           2  \n",
       "8           2  \n",
       "9           2  \n",
       "10          2  \n",
       "11          2  \n",
       "12          2  \n",
       "13          2  \n",
       "14          2  \n",
       "15          2  \n",
       "16          2  \n",
       "17          2  \n",
       "18          2  \n",
       "19          2  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First impressions of the present data\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataframe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment distribution\n",
    "# 0 - negative 1 - somewhat negative 2 - neutral 3 - somewhat positive 4 - positive\n",
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Visualize sentiment distribution \n",
    "Sentiment_count=data.groupby('Sentiment').count()\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['Phrase'])\n",
    "plt.xlabel('Review Sentiments')\n",
    "plt.ylabel('Number of Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #Convert a collection of text documents to a matrix of token counts\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# lowercase=True - convert all characters to lowercase\n",
    "# stop_words='englisch' - remove all stopwords based on the english language\n",
    "# ngram_range = (1,1) - We just  \n",
    "# Word Tokenizer\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1), tokenizer = word_tokenize)\n",
    "text_counts= cv.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 28)\t1\n",
      "  (0, 13713)\t1\n",
      "  (0, 707)\t1\n",
      "  (0, 716)\t1\n",
      "  (0, 9800)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 5904)\t1\n",
      "  (0, 6169)\t1\n",
      "  (0, 6145)\t2\n",
      "  (0, 397)\t1\n",
      "  (0, 3745)\t1\n",
      "  (0, 4852)\t1\n",
      "  (0, 12632)\t1\n",
      "  (1, 6169)\t1\n",
      "  (1, 6145)\t1\n",
      "  (1, 397)\t1\n",
      "  (1, 3745)\t1\n",
      "  (1, 4852)\t1\n",
      "  (1, 12632)\t1\n",
      "  (2, 12632)\t1\n",
      "  (4, 12632)\t1\n",
      "  (5, 6169)\t1\n",
      "  (5, 6145)\t1\n",
      "  (5, 397)\t1\n",
      "  (5, 3745)\t1\n",
      "  :\t:\n",
      "  (156050, 9789)\t1\n",
      "  (156050, 12191)\t1\n",
      "  (156051, 9789)\t1\n",
      "  (156051, 12191)\t1\n",
      "  (156052, 12191)\t1\n",
      "  (156053, 2504)\t1\n",
      "  (156053, 1152)\t1\n",
      "  (156053, 6625)\t1\n",
      "  (156053, 5619)\t1\n",
      "  (156053, 1452)\t1\n",
      "  (156053, 19)\t1\n",
      "  (156054, 2504)\t1\n",
      "  (156054, 1152)\t1\n",
      "  (156054, 6625)\t1\n",
      "  (156054, 5619)\t1\n",
      "  (156054, 19)\t1\n",
      "  (156055, 6625)\t1\n",
      "  (156055, 19)\t1\n",
      "  (156056, 2504)\t1\n",
      "  (156056, 1152)\t1\n",
      "  (156056, 5619)\t1\n",
      "  (156057, 2504)\t1\n",
      "  (156057, 1152)\t1\n",
      "  (156058, 1152)\t1\n",
      "  (156059, 2504)\t1\n"
     ]
    }
   ],
   "source": [
    "# Term Document Matrix (document, term) - \"total count\"\n",
    "print(text_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, data['Sentiment'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.6095518817548806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TF-IDF instead of Total Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf=TfidfVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = word_tokenize)\n",
    "text_tf= tf.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, data['Sentiment'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for TF-IDF Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.5904780212738691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
